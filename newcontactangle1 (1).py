# -*- coding: utf-8 -*-
"""NewContactAngle1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Bqyd6_gxsxYp7OgOp3-Jx3MiPjsZWGVz

# Importing Libraries
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor

"""# Importing Dataset"""

import pandas as pd
df = pd.read_csv('download.csv')
df

df.info()

df.shape

"""# Cleaning Data"""

df = df.dropna().reset_index(drop=True)
df

df.shape

"""# Feature Selection"""

# Define features and target
x = df.drop(columns=["Contact angle"])
y = df["Contact angle"]

x

y

# Define numeric and categorical columns
numeric_features = ["Texture diameter", "Texture length", "Texture depth", "Texture pitch", "Roughness factor"]
categorical_features = ["Texture shape"]

"""# Finding Correlations"""

# Define numeric columns
numeric_columns = ["Texture diameter", "Texture length", "Texture depth", "Texture pitch", "Roughness factor", "Contact angle"]

corr_matrix = df[numeric_columns].corr()
corr_matrix['Contact angle'].sort_values(ascending=False)

"""# Creating Column Transformer"""

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(), categorical_features)
    ])

"""# Creating Pipeline"""

# Import different models
pipelines = {
    "Linear Regression": Pipeline([
        ("preprocessor", preprocessor),
        ("model", LinearRegression())
    ]),
    "KNN Regressor": Pipeline([
        ("preprocessor", preprocessor),
        ("model", KNeighborsRegressor(n_neighbors=5))
    ]),
    "Gradient Boosting": Pipeline([
        ("preprocessor", preprocessor),
        ("model", GradientBoostingRegressor(random_state=42))
    ]),
    "Random Forest": Pipeline([
        ("preprocessor", preprocessor),
        ("model", RandomForestRegressor(n_estimators=100, random_state=42))
    ])
}

for name, pipeline in pipelines.items():
    scores = cross_val_score(pipeline, x, y, cv=5, scoring="r2")
    print(f"{name} -> Mean R²: {scores.mean():.4f}, Std Dev: {scores.std():.4f}")

from sklearn.metrics import mean_squared_error, make_scorer
import numpy as np

# Custom scorers for MSE and RMSE (note sklearn doesn't have built-in RMSE scorer)
mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)  # negative MSE for cross_val_score
rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)

for name, pipeline in pipelines.items():
    r2_scores = cross_val_score(pipeline, x, y, cv=5, scoring="r2")
    neg_mse_scores = cross_val_score(pipeline, x, y, cv=5, scoring="neg_mean_squared_error")
    # RMSE can be derived from neg_mse_scores
    rmse_scores = np.sqrt(-neg_mse_scores)

    print(f"{name} -> Mean R²: {r2_scores.mean():.4f} ± {r2_scores.std():.4f}")
    print(f"{name} -> Mean MSE: {-neg_mse_scores.mean():.4f} ± {neg_mse_scores.std():.4f}")
    print(f"{name} -> Mean RMSE: {rmse_scores.mean():.4f} ± {rmse_scores.std():.4f}")
    print()

"""# Train-Test Split using Linear Regression"""

from sklearn.model_selection import train_test_split

# Pehle X (features) aur y (target) define karo
X = df.drop(columns=["Contact angle"])
y = df["Contact angle"]

# Ab train-test split (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train_processed = preprocessor.fit_transform(X_train)
X_test_processed = preprocessor.transform(X_test)

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
lr=LinearRegression()
lr.fit(X_train_processed,y_train)
y_lr_train_pred=lr.predict(X_train_processed)
y_lr_test_pred=lr.predict(X_test_processed)

from sklearn.metrics import mean_squared_error, r2_score
lr_train_mse = mean_squared_error(y_train, y_lr_train_pred)
lr_train_r2 = r2_score(y_train, y_lr_train_pred)

lr_test_mse = mean_squared_error(y_test, y_lr_test_pred)
lr_test_r2 = r2_score(y_test, y_lr_test_pred)

print('LR MSE (Train): ', lr_train_mse)
print('LR R2 (Train): ', lr_train_r2)
print('LR MSE (Test): ', lr_test_mse)
print('LR R2 (Test): ', lr_test_r2)

"""# Train-Test Split using Gradient Boosting"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pick a model pipeline
model_name = "Gradient Boosting"
model_pipeline = pipelines[model_name]

# Fit the model
model_pipeline.fit(X_train, y_train)

# Predict
y_pred = model_pipeline.predict(X_test)

# Compare actual vs predicted
comparison_df = pd.DataFrame({
    "Actual": y_test.values,
    "Predicted": y_pred
})
print(comparison_df.head(10))  # Show first 10 rows

# prompt: predict with random values

# Generate random values for prediction (using the same feature names)
random_data = {
    "Texture diameter": [np.random.uniform(10, 50)],  # Example range
    "Texture length": [np.random.uniform(10, 50)],    # Example range
    "Texture depth": [np.random.uniform(1, 10)],      # Example range
    "Texture pitch": [np.random.uniform(10, 50)],     # Example range
    "Roughness factor": [np.random.uniform(1, 2)],   # Example range
    "Texture shape": [random.choice(["", "", "Triangle"])] # Example choices
}

random_df = pd.DataFrame(random_data)

# Use the trained model pipeline to predict on the random data
random_prediction = model_pipeline.predict(random_df)

print(f"\nPrediction for random data: {random_prediction[0]:.4f}")